 CUDA-version: 10010 (10020), cuDNN: 7.6.4, CUDNN_HALF=1, GPU count: 8  
 CUDNN_HALF=1 
 OpenCV version: 4.9.1
 compute_capability = 750, cudnn_half = 1 
net.optimized_memory = 0 
mini_batch = 1, batch = 1, time_steps = 1, train = 0 
   layer   filters  size/strd(dil)      input                output
   0 conv     32       3 x 3/ 2    416 x 416 x   3 ->  208 x 208 x  32 0.075 BF
   1 conv     32       1 x 1/ 1    208 x 208 x  32 ->  208 x 208 x  32 0.089 BF
   2 conv     32/  32  3 x 3/ 1    208 x 208 x  32 ->  208 x 208 x  32 0.025 BF
   3 conv     16       1 x 1/ 1    208 x 208 x  32 ->  208 x 208 x  16 0.044 BF
   4 conv     96       1 x 1/ 1    208 x 208 x  16 ->  208 x 208 x  96 0.133 BF
   5 conv     96/  96  3 x 3/ 2    208 x 208 x  96 ->  104 x 104 x  96 0.019 BF
   6 conv     24       1 x 1/ 1    104 x 104 x  96 ->  104 x 104 x  24 0.050 BF
   7 conv    144       1 x 1/ 1    104 x 104 x  24 ->  104 x 104 x 144 0.075 BF
   8 conv    144/ 144  3 x 3/ 1    104 x 104 x 144 ->  104 x 104 x 144 0.028 BF
   9 conv     24       1 x 1/ 1    104 x 104 x 144 ->  104 x 104 x  24 0.075 BF
  10 activation: Using default 'linear'
Shortcut Layer: 6,  wt = 0, wn = 0, outputs: 104 x 104 x  24 0.000 BF
  11 conv    144       1 x 1/ 1    104 x 104 x  24 ->  104 x 104 x 144 0.075 BF
  12 conv    144/ 144  3 x 3/ 2    104 x 104 x 144 ->   52 x  52 x 144 0.007 BF
  13 conv     32       1 x 1/ 1     52 x  52 x 144 ->   52 x  52 x  32 0.025 BF
  14 conv    192       1 x 1/ 1     52 x  52 x  32 ->   52 x  52 x 192 0.033 BF
  15 conv    192/ 192  3 x 3/ 1     52 x  52 x 192 ->   52 x  52 x 192 0.009 BF
  16 conv     32       1 x 1/ 1     52 x  52 x 192 ->   52 x  52 x  32 0.033 BF
  17 activation: Using default 'linear'
Shortcut Layer: 13,  wt = 0, wn = 0, outputs:  52 x  52 x  32 0.000 BF
  18 conv    192       1 x 1/ 1     52 x  52 x  32 ->   52 x  52 x 192 0.033 BF
  19 conv    192/ 192  3 x 3/ 1     52 x  52 x 192 ->   52 x  52 x 192 0.009 BF
  20 conv     32       1 x 1/ 1     52 x  52 x 192 ->   52 x  52 x  32 0.033 BF
  21 activation: Using default 'linear'
Shortcut Layer: 17,  wt = 0, wn = 0, outputs:  52 x  52 x  32 0.000 BF
  22 conv    192       1 x 1/ 1     52 x  52 x  32 ->   52 x  52 x 192 0.033 BF
  23 conv    192/ 192  3 x 3/ 1     52 x  52 x 192 ->   52 x  52 x 192 0.009 BF
  24 conv     64       1 x 1/ 1     52 x  52 x 192 ->   52 x  52 x  64 0.066 BF
  25 conv    384       1 x 1/ 1     52 x  52 x  64 ->   52 x  52 x 384 0.133 BF
  26 conv    384/ 384  3 x 3/ 1     52 x  52 x 384 ->   52 x  52 x 384 0.019 BF
  27 conv     64       1 x 1/ 1     52 x  52 x 384 ->   52 x  52 x  64 0.133 BF
  28 activation: Using default 'linear'
Shortcut Layer: 24,  wt = 0, wn = 0, outputs:  52 x  52 x  64 0.000 BF
  29 conv    384       1 x 1/ 1     52 x  52 x  64 ->   52 x  52 x 384 0.133 BF
  30 conv    384/ 384  3 x 3/ 1     52 x  52 x 384 ->   52 x  52 x 384 0.019 BF
  31 conv     64       1 x 1/ 1     52 x  52 x 384 ->   52 x  52 x  64 0.133 BF
  32 activation: Using default 'linear'
Shortcut Layer: 28,  wt = 0, wn = 0, outputs:  52 x  52 x  64 0.000 BF
  33 conv    384       1 x 1/ 1     52 x  52 x  64 ->   52 x  52 x 384 0.133 BF
  34 conv    384/ 384  3 x 3/ 1     52 x  52 x 384 ->   52 x  52 x 384 0.019 BF
  35 conv     64       1 x 1/ 1     52 x  52 x 384 ->   52 x  52 x  64 0.133 BF
  36 activation: Using default 'linear'
Shortcut Layer: 32,  wt = 0, wn = 0, outputs:  52 x  52 x  64 0.000 BF
  37 conv    384       1 x 1/ 1     52 x  52 x  64 ->   52 x  52 x 384 0.133 BF
  38 conv    384/ 384  3 x 3/ 2     52 x  52 x 384 ->   26 x  26 x 384 0.005 BF
  39 conv     96       1 x 1/ 1     26 x  26 x 384 ->   26 x  26 x  96 0.050 BF
  40 conv    576       1 x 1/ 1     26 x  26 x  96 ->   26 x  26 x 576 0.075 BF
  41 conv    576/ 576  3 x 3/ 1     26 x  26 x 576 ->   26 x  26 x 576 0.007 BF
  42 conv     96       1 x 1/ 1     26 x  26 x 576 ->   26 x  26 x  96 0.075 BF
  43 activation: Using default 'linear'
Shortcut Layer: 39,  wt = 0, wn = 0, outputs:  26 x  26 x  96 0.000 BF
  44 conv    576       1 x 1/ 1     26 x  26 x  96 ->   26 x  26 x 576 0.075 BF
  45 conv    576/ 576  3 x 3/ 1     26 x  26 x 576 ->   26 x  26 x 576 0.007 BF
  46 conv     96       1 x 1/ 1     26 x  26 x 576 ->   26 x  26 x  96 0.075 BF
  47 activation: Using default 'linear'
Shortcut Layer: 43,  wt = 0, wn = 0, outputs:  26 x  26 x  96 0.000 BF
  48 conv    576       1 x 1/ 1     26 x  26 x  96 ->   26 x  26 x 576 0.075 BF
  49 conv    576/ 576  3 x 3/ 2     26 x  26 x 576 ->   13 x  13 x 576 0.002 BF
  50 conv    160       1 x 1/ 1     13 x  13 x 576 ->   13 x  13 x 160 0.031 BF
  51 conv    960       1 x 1/ 1     13 x  13 x 160 ->   13 x  13 x 960 0.052 BF
  52 conv    960/ 960  3 x 3/ 1     13 x  13 x 960 ->   13 x  13 x 960 0.003 BF
  53 conv    160       1 x 1/ 1     13 x  13 x 960 ->   13 x  13 x 160 0.052 BF
  54 activation: Using default 'linear'
Shortcut Layer: 50,  wt = 0, wn = 0, outputs:  13 x  13 x 160 0.000 BF
  55 conv    960       1 x 1/ 1     13 x  13 x 160 ->   13 x  13 x 960 0.052 BF
  56 conv    960/ 960  3 x 3/ 1     13 x  13 x 960 ->   13 x  13 x 960 0.003 BF
  57 conv    160       1 x 1/ 1     13 x  13 x 960 ->   13 x  13 x 160 0.052 BF
  58 activation: Using default 'linear'
Shortcut Layer: 54,  wt = 0, wn = 0, outputs:  13 x  13 x 160 0.000 BF
  59 max                5x 5/ 1     13 x  13 x 160 ->   13 x  13 x 160 0.001 BF
  60 route  58 		                           ->   13 x  13 x 160 
  61 max                9x 9/ 1     13 x  13 x 160 ->   13 x  13 x 160 0.002 BF
  62 route  58 		                           ->   13 x  13 x 160 
  63 max               13x13/ 1     13 x  13 x 160 ->   13 x  13 x 160 0.005 BF
  64 route  63 61 59 58 	                   ->   13 x  13 x 640 
  65 conv    160       1 x 1/ 1     13 x  13 x 640 ->   13 x  13 x 160 0.035 BF
  66 conv    960       1 x 1/ 1     13 x  13 x 160 ->   13 x  13 x 960 0.052 BF
  67 conv    960/ 960  3 x 3/ 1     13 x  13 x 960 ->   13 x  13 x 960 0.003 BF
  68 conv    160       1 x 1/ 1     13 x  13 x 960 ->   13 x  13 x 160 0.052 BF
  69 activation: Using default 'linear'
Shortcut Layer: 65,  wt = 0, wn = 0, outputs:  13 x  13 x 160 0.000 BF
  70 conv    960       1 x 1/ 1     13 x  13 x 160 ->   13 x  13 x 960 0.052 BF
  71 conv    960/ 960  3 x 3/ 1     13 x  13 x 960 ->   13 x  13 x 960 0.003 BF
  72 conv    320       1 x 1/ 1     13 x  13 x 960 ->   13 x  13 x 320 0.104 BF
  73 conv   1280       1 x 1/ 1     13 x  13 x 320 ->   13 x  13 x1280 0.138 BF
  74 conv    255       1 x 1/ 1     13 x  13 x1280 ->   13 x  13 x 255 0.110 BF
  75 yolo
[yolo] params: iou loss: ciou (4), iou_norm: 0.07, cls_norm: 1.00, scale_x_y: 1.05
nms_kind: greedynms (1), beta = 0.600000 
  76 route  70 		                           ->   13 x  13 x 960 
  77 conv    256       1 x 1/ 1     13 x  13 x 960 ->   13 x  13 x 256 0.083 BF
  78 upsample                 2x    13 x  13 x 256 ->   26 x  26 x 256
  79 route  78 48 	                           ->   26 x  26 x 832 
  80 conv     96       1 x 1/ 1     26 x  26 x 832 ->   26 x  26 x  96 0.108 BF
  81 conv    576       1 x 1/ 1     26 x  26 x  96 ->   26 x  26 x 576 0.075 BF
  82 conv    576/ 576  3 x 3/ 1     26 x  26 x 576 ->   26 x  26 x 576 0.007 BF
  83 conv     96       1 x 1/ 1     26 x  26 x 576 ->   26 x  26 x  96 0.075 BF
  84 activation: Using default 'linear'
Shortcut Layer: 80,  wt = 0, wn = 0, outputs:  26 x  26 x  96 0.000 BF
  85 conv    576       1 x 1/ 1     26 x  26 x  96 ->   26 x  26 x 576 0.075 BF
  86 conv    576/ 576  3 x 3/ 1     26 x  26 x 576 ->   26 x  26 x 576 0.007 BF
  87 conv     96       1 x 1/ 1     26 x  26 x 576 ->   26 x  26 x  96 0.075 BF
  88 activation: Using default 'linear'
Shortcut Layer: 84,  wt = 0, wn = 0, outputs:  26 x  26 x  96 0.000 BF
  89 conv    480       1 x 1/ 1     26 x  26 x  96 ->   26 x  26 x 480 0.062 BF
  90 conv    480/ 480  3 x 3/ 1     26 x  26 x 480 ->   26 x  26 x 480 0.006 BF
  91 conv    160       1 x 1/ 1     26 x  26 x 480 ->   26 x  26 x 160 0.104 BF
  92 conv    640       1 x 1/ 1     26 x  26 x 160 ->   26 x  26 x 640 0.138 BF
  93 conv    255       1 x 1/ 1     26 x  26 x 640 ->   26 x  26 x 255 0.221 BF
  94 yolo
[yolo] params: iou loss: ciou (4), iou_norm: 0.07, cls_norm: 1.00, scale_x_y: 1.10
nms_kind: greedynms (1), beta = 0.600000 
  95 route  89 		                           ->   26 x  26 x 480 
  96 conv    128       1 x 1/ 1     26 x  26 x 480 ->   26 x  26 x 128 0.083 BF
  97 upsample                 2x    26 x  26 x 128 ->   52 x  52 x 128
  98 route  97 37 	                           ->   52 x  52 x 512 
  99 conv     64       1 x 1/ 1     52 x  52 x 512 ->   52 x  52 x  64 0.177 BF
 100 conv    384       1 x 1/ 1     52 x  52 x  64 ->   52 x  52 x 384 0.133 BF
 101 conv    384/ 384  3 x 3/ 1     52 x  52 x 384 ->   52 x  52 x 384 0.019 BF
 102 conv     64       1 x 1/ 1     52 x  52 x 384 ->   52 x  52 x  64 0.133 BF
 103 activation: Using default 'linear'
Shortcut Layer: 99,  wt = 0, wn = 0, outputs:  52 x  52 x  64 0.000 BF
 104 conv     64       1 x 1/ 1     52 x  52 x  64 ->   52 x  52 x  64 0.022 BF
 105 conv    384       1 x 1/ 1     52 x  52 x  64 ->   52 x  52 x 384 0.133 BF
 106 conv    384/ 384  3 x 3/ 1     52 x  52 x 384 ->   52 x  52 x 384 0.019 BF
 107 conv     64       1 x 1/ 1     52 x  52 x 384 ->   52 x  52 x  64 0.133 BF
 108 activation: Using default 'linear'
Shortcut Layer: 104,  wt = 0, wn = 0, outputs:  52 x  52 x  64 0.000 BF
 109 conv     64       1 x 1/ 1     52 x  52 x  64 ->   52 x  52 x  64 0.022 BF
 110 conv    384       1 x 1/ 1     52 x  52 x  64 ->   52 x  52 x 384 0.133 BF
 111 conv    384/ 384  3 x 3/ 1     52 x  52 x 384 ->   52 x  52 x 384 0.019 BF
 112 conv     64       1 x 1/ 1     52 x  52 x 384 ->   52 x  52 x  64 0.133 BF
 113 activation: Using default 'linear'
Shortcut Layer: 109,  wt = 0, wn = 0, outputs:  52 x  52 x  64 0.000 BF
 114 conv    240       1 x 1/ 1     52 x  52 x  64 ->   52 x  52 x 240 0.083 BF
 115 conv    240/ 240  3 x 3/ 1     52 x  52 x 240 ->   52 x  52 x 240 0.012 BF
 116 conv     80       1 x 1/ 1     52 x  52 x 240 ->   52 x  52 x  80 0.104 BF
 117 conv    320       1 x 1/ 1     52 x  52 x  80 ->   52 x  52 x 320 0.138 BF
 118 conv    255       1 x 1/ 1     52 x  52 x 320 ->   52 x  52 x 255 0.441 BF
 119 yolo
[yolo] params: iou loss: ciou (4), iou_norm: 0.07, cls_norm: 1.00, scale_x_y: 1.20
nms_kind: greedynms (1), beta = 0.600000 
Total BFLOPS 6.184 
avg_outputs = 422440 
 Allocate additional workspace_size = 9.95 MB 
Loading weights from yolov3-spp-mobilenetv2-coco_best.weights...
 seen 64, trained: 8711 K-images (136 Kilo-batches_64) 
Done! Loaded 120 layers from weights-file 

 calculation mAP (mean average precision)...
4476
 detections_count = 285474, unique_truth_count = 25679  
class_id = 0, name = person, ap = 70.98%   	 (TP = 4697, FP = 1837) 
class_id = 1, name = bicycle, ap = 38.84%   	 (TP = 79, FP = 62) 
class_id = 2, name = car, ap = 51.12%   	 (TP = 777, FP = 564) 
class_id = 3, name = motorbike, ap = 0.00%   	 (TP = 0, FP = 0) 
class_id = 4, name = aeroplane, ap = 0.00%   	 (TP = 0, FP = 0) 
class_id = 5, name = bus, ap = 74.25%   	 (TP = 150, FP = 74) 
class_id = 6, name = train, ap = 76.97%   	 (TP = 129, FP = 38) 
class_id = 7, name = truck, ap = 41.25%   	 (TP = 157, FP = 195) 
class_id = 8, name = boat, ap = 26.41%   	 (TP = 82, FP = 121) 
class_id = 9, name = traffic light, ap = 37.83%   	 (TP = 208, FP = 183) 
class_id = 10, name = fire hydrant, ap = 71.95%   	 (TP = 67, FP = 18) 
class_id = 11, name = stop sign, ap = 63.09%   	 (TP = 45, FP = 23) 
class_id = 12, name = parking meter, ap = 48.94%   	 (TP = 24, FP = 11) 
class_id = 13, name = bench, ap = 26.55%   	 (TP = 83, FP = 125) 
class_id = 14, name = bird, ap = 46.83%   	 (TP = 94, FP = 47) 
class_id = 15, name = cat, ap = 82.84%   	 (TP = 155, FP = 65) 
class_id = 16, name = dog, ap = 73.72%   	 (TP = 143, FP = 82) 
class_id = 17, name = horse, ap = 66.90%   	 (TP = 148, FP = 77) 
class_id = 18, name = sheep, ap = 59.23%   	 (TP = 150, FP = 151) 
class_id = 19, name = cow, ap = 59.45%   	 (TP = 140, FP = 87) 
class_id = 20, name = elephant, ap = 76.52%   	 (TP = 156, FP = 66) 
class_id = 21, name = bear, ap = 81.84%   	 (TP = 53, FP = 12) 
class_id = 22, name = zebra, ap = 81.19%   	 (TP = 175, FP = 54) 
class_id = 23, name = giraffe, ap = 83.29%   	 (TP = 179, FP = 30) 
class_id = 24, name = backpack, ap = 16.39%   	 (TP = 52, FP = 76) 
class_id = 25, name = umbrella, ap = 46.16%   	 (TP = 119, FP = 74) 
class_id = 26, name = handbag, ap = 8.77%   	 (TP = 42, FP = 129) 
class_id = 27, name = tie, ap = 46.70%   	 (TP = 78, FP = 64) 
class_id = 28, name = suitcase, ap = 35.17%   	 (TP = 68, FP = 60) 
class_id = 29, name = frisbee, ap = 64.58%   	 (TP = 68, FP = 37) 
class_id = 30, name = skis, ap = 35.96%   	 (TP = 70, FP = 72) 
class_id = 31, name = snowboard, ap = 39.03%   	 (TP = 21, FP = 33) 
class_id = 32, name = sports ball, ap = 51.06%   	 (TP = 95, FP = 62) 
class_id = 33, name = kite, ap = 50.99%   	 (TP = 75, FP = 59) 
class_id = 34, name = baseball bat, ap = 27.77%   	 (TP = 34, FP = 48) 
class_id = 35, name = baseball glove, ap = 44.99%   	 (TP = 52, FP = 41) 
class_id = 36, name = skateboard, ap = 57.60%   	 (TP = 90, FP = 47) 
class_id = 37, name = surfboard, ap = 45.39%   	 (TP = 108, FP = 102) 
class_id = 38, name = tennis racket, ap = 59.63%   	 (TP = 102, FP = 58) 
class_id = 39, name = bottle, ap = 40.81%   	 (TP = 293, FP = 260) 
class_id = 40, name = wine glass, ap = 36.47%   	 (TP = 82, FP = 86) 
class_id = 41, name = cup, ap = 41.19%   	 (TP = 295, FP = 290) 
class_id = 42, name = fork, ap = 27.49%   	 (TP = 50, FP = 64) 
class_id = 43, name = knife, ap = 10.58%   	 (TP = 31, FP = 78) 
class_id = 44, name = spoon, ap = 10.91%   	 (TP = 24, FP = 49) 
class_id = 45, name = bowl, ap = 43.03%   	 (TP = 216, FP = 213) 
class_id = 46, name = banana, ap = 23.21%   	 (TP = 54, FP = 89) 
class_id = 47, name = apple, ap = 15.04%   	 (TP = 37, FP = 78) 
class_id = 48, name = sandwich, ap = 42.47%   	 (TP = 73, FP = 73) 
class_id = 49, name = orange, ap = 31.80%   	 (TP = 90, FP = 115) 
class_id = 50, name = broccoli, ap = 30.41%   	 (TP = 74, FP = 77) 
class_id = 51, name = carrot, ap = 19.88%   	 (TP = 53, FP = 79) 
class_id = 52, name = hot dog, ap = 47.13%   	 (TP = 37, FP = 23) 
class_id = 53, name = pizza, ap = 66.37%   	 (TP = 160, FP = 76) 
class_id = 54, name = donut, ap = 48.83%   	 (TP = 99, FP = 71) 
class_id = 55, name = cake, ap = 43.31%   	 (TP = 86, FP = 72) 
class_id = 56, name = chair, ap = 32.64%   	 (TP = 363, FP = 371) 
class_id = 57, name = sofa, ap = 0.00%   	 (TP = 0, FP = 0) 
class_id = 58, name = pottedplant, ap = 0.00%   	 (TP = 0, FP = 0) 
class_id = 59, name = bed, ap = 54.64%   	 (TP = 81, FP = 83) 
class_id = 60, name = diningtable, ap = 0.00%   	 (TP = 0, FP = 0) 
class_id = 61, name = toilet, ap = 70.97%   	 (TP = 118, FP = 50) 
class_id = 62, name = tvmonitor, ap = 0.00%   	 (TP = 0, FP = 0) 
class_id = 63, name = laptop, ap = 62.86%   	 (TP = 126, FP = 66) 
class_id = 64, name = mouse, ap = 68.14%   	 (TP = 65, FP = 35) 
class_id = 65, name = remote, ap = 21.75%   	 (TP = 54, FP = 83) 
class_id = 66, name = keyboard, ap = 60.90%   	 (TP = 88, FP = 73) 
class_id = 67, name = cell phone, ap = 35.47%   	 (TP = 80, FP = 70) 
class_id = 68, name = microwave, ap = 63.94%   	 (TP = 31, FP = 26) 
class_id = 69, name = oven, ap = 43.26%   	 (TP = 57, FP = 54) 
class_id = 70, name = toaster, ap = 16.68%   	 (TP = 1, FP = 0) 
class_id = 71, name = sink, ap = 44.73%   	 (TP = 103, FP = 78) 
class_id = 72, name = refrigerator, ap = 58.21%   	 (TP = 61, FP = 26) 
class_id = 73, name = book, ap = 11.96%   	 (TP = 108, FP = 304) 
class_id = 74, name = clock, ap = 66.26%   	 (TP = 151, FP = 65) 
class_id = 75, name = vase, ap = 37.56%   	 (TP = 83, FP = 64) 
class_id = 76, name = scissors, ap = 33.48%   	 (TP = 11, FP = 8) 
class_id = 77, name = teddy bear, ap = 61.27%   	 (TP = 86, FP = 43) 
class_id = 78, name = hair drier, ap = 0.53%   	 (TP = 0, FP = 0) 
class_id = 79, name = toothbrush, ap = 13.65%   	 (TP = 8, FP = 17) 

 for conf_thresh = 0.25, precision = 0.60, recall = 0.48, F1-score = 0.54 
 for conf_thresh = 0.25, TP = 12394, FP = 8193, FN = 13285, average IoU = 46.11 % 

 IoU threshold = 50 %, used Area-Under-Curve for each unique Recall 
 mean average precision (mAP@0.50) = 0.426003, or 42.60 % 
Total Detection Time: 68 Seconds
